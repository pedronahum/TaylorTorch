import _Differentiation
import Foundation

/// Adaptive Moment Estimation (Adam) optimizer with optional AdamW weight decay.
///
/// Adam is an adaptive learning rate optimizer that computes individual learning rates
/// for different parameters. It combines the benefits of RMSProp and momentum, making it
/// highly effective for training transformers, RNNs, and models with sparse gradients.
///
/// ## Basic Usage
///
/// ```swift
/// // Create transformer model
/// var model = Sequential {
///     Embedding(vocabularySize: 50000, embeddingSize: 768)
///     TransformerEncoderLayer(modelDim: 768, numHeads: 12)
///     Dense(inputSize: 768, outputSize: 2)
/// }
///
/// // Initialize Adam optimizer
/// var optimizer = Adam(
///     for: model,
///     learningRate: 1e-3,  // Default works well for most cases
///     beta1: 0.9,
///     beta2: 0.999
/// )
///
/// // Training step
/// let (loss, gradients) = valueWithGradient(at: model) { model -> Tensor in
///     let predictions = model(tokens)
///     return crossEntropyLoss(predictions, labels)
/// }
/// optimizer.update(&model, along: gradients)
/// ```
///
/// ## Algorithm
///
/// Adam maintains two moving averages for each parameter:
///
/// ```
/// m_t = β1 * m_{t-1} + (1 - β1) * ∇L        // First moment (mean)
/// v_t = β2 * v_{t-1} + (1 - β2) * (∇L)²    // Second moment (variance)
///
/// m̂_t = m_t / (1 - β1^t)                    // Bias correction
/// v̂_t = v_t / (1 - β2^t)
///
/// θ_t+1 = θ_t - α * m̂_t / (√v̂_t + ε)      // Parameter update
/// ```
///
/// Where:
/// - `θ` = model parameters
/// - `α` = learning rate (default: 0.001)
/// - `β1` = first moment decay (default: 0.9)
/// - `β2` = second moment decay (default: 0.999)
/// - `ε` = numerical stability constant (default: 1e-8)
/// - `∇L` = gradient of loss
///
/// ## Adam vs AdamW
///
/// This implementation supports both standard Adam and AdamW (decoupled weight decay):
///
/// ```swift
/// // Standard Adam - no weight decay
/// var adam = Adam(
///     for: model,
///     learningRate: 1e-3,
///     weightDecay: 0,
///     adamW: false
/// )
///
/// // AdamW - decoupled weight decay (RECOMMENDED)
/// var adamW = Adam(
///     for: model,
///     learningRate: 1e-3,
///     weightDecay: 0.01,  // Typical: 0.01 to 0.1
///     adamW: true         // Default is true
/// )
/// ```
///
/// **AdamW vs Adam weight decay:**
///
/// | Aspect | Adam (L2 Regularization) | AdamW (Decoupled) |
/// |--------|-------------------------|-------------------|
/// | Implementation | Adds gradient penalty | Directly decays weights |
/// | Effectiveness | Less effective | More effective |
/// | Recommended | No | Yes (default in TaylorTorch) |
/// | Used in | Older papers | Modern transformers (BERT, GPT) |
///
/// **Always use AdamW for transformers!**
///
/// ## Training Transformers with Adam
///
/// Adam is the standard optimizer for transformer models:
///
/// ```swift
/// // BERT-style model training
/// struct BERTClassifier: Layer {
///     var embeddings: Embedding
///     var encoders: [TransformerEncoderLayer]
///     var classifier: Dense
///
///     init(vocabSize: Int, modelDim: Int, numHeads: Int, numLayers: Int) {
///         embeddings = Embedding(vocabularySize: vocabSize, embeddingSize: modelDim)
///         encoders = (0..<numLayers).map { _ in
///             TransformerEncoderLayer(modelDim: modelDim, numHeads: numHeads)
///         }
///         classifier = Dense(inputSize: modelDim, outputSize: 2)
///     }
///
///     @differentiable
///     func callAsFunction(_ input: Tensor) -> Tensor {
///         var x = embeddings(input)
///         for encoder in encoders {
///             x = encoder(x)
///         }
///         return classifier(x[:, 0, :])  // Use [CLS] token
///     }
/// }
///
/// // Initialize with transformer-standard hyperparameters
/// var model = BERTClassifier(
///     vocabSize: 30000,
///     modelDim: 768,
///     numHeads: 12,
///     numLayers: 12
/// )
///
/// var optimizer = Adam(
///     for: model,
///     learningRate: 1e-4,    // Lower LR for fine-tuning
///     beta1: 0.9,            // Standard
///     beta2: 0.999,          // Standard
///     weightDecay: 0.01,     // Regularization
///     adamW: true            // Use AdamW
/// )
///
/// // Training loop with learning rate warmup
/// let warmupSteps = 10000
/// let totalSteps = 100000
/// let peakLR: Float = 1e-4
///
/// for step in 1...totalSteps {
///     // Linear warmup, then linear decay
///     if step <= warmupSteps {
///         optimizer.learningRate = peakLR * Float(step) / Float(warmupSteps)
///     } else {
///         let decaySteps = Float(totalSteps - warmupSteps)
///         optimizer.learningRate = peakLR * Float(totalSteps - step) / decaySteps
///     }
///
///     // Training step
///     let (loss, gradients) = valueWithGradient(at: model) { model -> Tensor in
///         let logits = model(inputIds)
///         return crossEntropyLoss(logits, labels)
///     }
///
///     optimizer.update(&model, along: gradients)
///
///     if step % 1000 == 0 {
///         print("Step \(step): LR = \(optimizer.learningRate), Loss = \(loss.item())")
///     }
/// }
/// ```
///
/// ## Hyperparameter Guidelines
///
/// | Model Type | Learning Rate | β1 | β2 | Weight Decay | AdamW |
/// |------------|---------------|-----|-----|--------------|--------|
/// | BERT (pretrain) | 1e-4 | 0.9 | 0.999 | 0.01 | true |
/// | BERT (fine-tune) | 2e-5 | 0.9 | 0.999 | 0.01 | true |
/// | GPT-2/GPT-3 | 6e-4 | 0.9 | 0.95 | 0.1 | true |
/// | Vision Transformer | 1e-3 | 0.9 | 0.999 | 0.1 | true |
/// | RNN/LSTM | 1e-3 | 0.9 | 0.999 | 0 | false |
/// | Small batch (<32) | 1e-4 | 0.9 | 0.999 | 0.01 | true |
///
/// **General rules:**
/// - Start with default hyperparameters (lr=1e-3, β1=0.9, β2=0.999)
/// - Use lower learning rates for fine-tuning (1e-5 to 1e-4)
/// - Enable AdamW with weight_decay=0.01 for transformers
/// - Use learning rate warmup (5-10% of total steps)
///
/// ## Learning Rate Scheduling
///
/// Adam benefits from learning rate scheduling:
///
/// ### Linear Warmup + Decay
/// ```swift
/// let warmupSteps = 10000
/// let totalSteps = 100000
/// let peakLR: Float = 1e-3
///
/// for step in 1...totalSteps {
///     if step <= warmupSteps {
///         // Linear warmup
///         optimizer.learningRate = peakLR * Float(step) / Float(warmupSteps)
///     } else {
///         // Linear decay
///         let progress = Float(step - warmupSteps) / Float(totalSteps - warmupSteps)
///         optimizer.learningRate = peakLR * (1.0 - progress)
///     }
///     // ... training ...
/// }
/// ```
///
/// ### Cosine Annealing
/// ```swift
/// import Foundation
///
/// let minLR: Float = 1e-5
/// let maxLR: Float = 1e-3
///
/// for step in 1...totalSteps {
///     let cosineDecay = 0.5 * (1.0 + cos(Float.pi * Float(step) / Float(totalSteps)))
///     optimizer.learningRate = minLR + (maxLR - minLR) * cosineDecay
///     // ... training ...
/// }
/// ```
///
/// ## Adam vs SGD
///
/// | Aspect | Adam | SGD |
/// |--------|------|-----|
/// | Convergence Speed | Fast initially | Slow initially |
/// | Final Performance | Good | Often better |
/// | Hyperparameter Sensitivity | Low (robust) | High (LR critical) |
/// | Memory Usage | High (2x parameters) | Low |
/// | Learning Rate | Adaptive per-parameter | Global |
/// | Best For | Transformers, RNNs, small batch | CNNs, large batch |
///
/// **Use Adam when:**
/// - Training Transformers (BERT, GPT, T5)
/// - Training RNNs/LSTMs
/// - Small batch sizes (<32)
/// - Sparse gradients
/// - You want robust defaults without tuning
/// - Time-to-convergence matters more than final accuracy
///
/// **Use SGD when:**
/// - Training CNNs (ResNet, VGG)
/// - Large batch sizes (≥128)
/// - You want the absolute best final accuracy
/// - You have time to tune hyperparameters
///
/// ## Vision Transformer Example
///
/// ```swift
/// // Vision Transformer for image classification
/// struct ViT: Layer {
///     var patchEmbedding: Conv2D
///     var positionEmbedding: Tensor
///     var encoders: [TransformerEncoderLayer]
///     var mlpHead: Dense
///
///     init(imageSize: Int, patchSize: Int, numClasses: Int, modelDim: Int) {
///         // Patch embedding: split image into patches
///         patchEmbedding = Conv2D(
///             inChannels: 3,
///             outChannels: modelDim,
///             kernelSize: (patchSize, patchSize),
///             stride: (patchSize, patchSize)
///         )
///
///         let numPatches = (imageSize / patchSize) * (imageSize / patchSize)
///         positionEmbedding = Tensor.randn([1, numPatches + 1, modelDim]) * 0.02
///
///         encoders = (0..<12).map { _ in
///             TransformerEncoderLayer(modelDim: modelDim, numHeads: 12)
///         }
///
///         mlpHead = Dense(inputSize: modelDim, outputSize: numClasses)
///     }
///
///     @differentiable
///     func callAsFunction(_ images: Tensor) -> Tensor {
///         // images: [batch, 3, 224, 224]
///         var x = patchEmbedding(images)  // [batch, modelDim, 14, 14]
///         x = x.flatten(startDim: 2).transpose(1, 2)  // [batch, 196, modelDim]
///
///         // Add [CLS] token
///         let clsToken = Tensor.zeros([x.shape[0], 1, x.shape[2]])
///         x = Tensor.cat([clsToken, x], dim: 1)  // [batch, 197, modelDim]
///
///         // Add position embeddings
///         x = x + positionEmbedding
///
///         // Transformer encoders
///         for encoder in encoders {
///             x = encoder(x)
///         }
///
///         // Classification head on [CLS] token
///         return mlpHead(x[:, 0, :])
///     }
/// }
///
/// // Initialize and train
/// var model = ViT(imageSize: 224, patchSize: 16, numClasses: 1000, modelDim: 768)
/// var optimizer = Adam(
///     for: model,
///     learningRate: 1e-3,
///     weightDecay: 0.1,  // Higher weight decay for ViT
///     adamW: true
/// )
/// ```
///
/// ## Tips for Success
///
/// 1. **Use AdamW by default**: Set `adamW: true` and `weightDecay: 0.01`
/// 2. **Learning rate warmup**: Essential for transformers (5-10k steps)
/// 3. **Default hyperparameters work well**: Start with lr=1e-3, β1=0.9, β2=0.999
/// 4. **Lower LR for fine-tuning**: Use 1e-5 to 1e-4 when fine-tuning pretrained models
/// 5. **Monitor gradient norms**: Clip if >1.0 for RNNs/LSTMs
/// 6. **Cosine schedule for long training**: Better than linear decay for 100+ epochs
///
/// ## References
///
/// - ["Adam: A Method for Stochastic Optimization"](https://arxiv.org/abs/1412.6980)
///   (Kingma and Ba, 2014) - Original Adam paper
/// - ["Decoupled Weight Decay Regularization"](https://arxiv.org/abs/1711.05101)
///   (Loshchilov and Hutter, 2017) - AdamW paper
/// - ["Attention Is All You Need"](https://arxiv.org/abs/1706.03762)
///   (Vaswani et al., 2017) - Uses Adam for transformers
/// - ["BERT: Pre-training of Deep Bidirectional Transformers"](https://arxiv.org/abs/1810.04805)
///   (Devlin et al., 2018) - Uses AdamW
///
/// ## See Also
///
/// - ``Optimizer`` - Base protocol for all optimizers
/// - ``SGD`` - Stochastic Gradient Descent optimizer
public final class Adam<Model: Differentiable & EuclideanDifferentiable>: Optimizer
where
  Model.TangentVector: VectorProtocol & KeyPathIterable & PointwiseMultiplicative,
  Model.TangentVector.VectorSpaceScalar == Float
{
  // Satisfy Optimizer associated types.
  public typealias Scalar = Float
  public typealias Model = Model

  // MARK: - Hyperparameters

  /// Base learning rate (α).
  public var learningRate: Float
  /// First moment decay (β₁).
  public var beta1: Float
  /// Second moment decay (β₂).
  public var beta2: Float
  /// Numerical stability constant (ε).
  public var epsilon: Float
  /// Optional learning‑rate decay (SGD‑style: α_t = α / (1 + decay * t)).
  public var decay: Float
  /// Weight decay coefficient. If `adamW` is `true`, uses decoupled weight decay (AdamW).
  public var weightDecay: Float
  /// If `true`, applies decoupled weight decay (AdamW); if `false`, uses no decay.
  public var adamW: Bool

  // MARK: - State

  /// First moment (per-parameter).
  public var m: Model.TangentVector
  /// Second raw moment (per-parameter).
  public var v: Model.TangentVector
  /// Time step (t ≥ 0).
  public var step: Int = 0

  // MARK: - Init

  /// Creates Adam with sensible defaults.
  public init(
    for _: __shared Model,
    learningRate: Float = 1e-3,
    beta1: Float = 0.9,
    beta2: Float = 0.999,
    epsilon: Float = 1e-8,
    decay: Float = 0,
    weightDecay: Float = 0,
    adamW: Bool = true
  ) {
    self.learningRate = learningRate
    self.beta1 = beta1
    self.beta2 = beta2
    self.epsilon = epsilon
    self.decay = decay
    self.weightDecay = weightDecay
    self.adamW = adamW

    // Start moments at zero (same structure as model tangents).
    self.m = .zero
    self.v = .zero
  }

  // MARK: - Device copy (matches your SGD pattern)

  /// Create a copy of this optimizer moving its state (m, v) to `device`.
  public required init(copying other: Adam, to device: Device) {
    self.learningRate = other.learningRate
    self.beta1 = other.beta1
    self.beta2 = other.beta2
    self.epsilon = other.epsilon
    self.decay = other.decay
    self.weightDecay = other.weightDecay
    self.adamW = other.adamW
    self.step = other.step

    // Move first/second moments to the requested device by visiting all Tensor leaves.
    var mCopy = other.m
    for kp in mCopy.recursivelyAllWritableKeyPaths(to: Tensor.self) {
      mCopy[keyPath: kp] = mCopy[keyPath: kp].to(device: device)
    }
    for kp in mCopy.recursivelyAllWritableKeyPaths(to: Tensor?.self) {
      if let t = mCopy[keyPath: kp] { mCopy[keyPath: kp] = t.to(device: device) }
    }
    for kp in mCopy.recursivelyAllWritableKeyPaths(to: [Tensor].self) {
      mCopy[keyPath: kp] = mCopy[keyPath: kp].map { $0.to(device: device) }
    }

    var vCopy = other.v
    for kp in vCopy.recursivelyAllWritableKeyPaths(to: Tensor.self) {
      vCopy[keyPath: kp] = vCopy[keyPath: kp].to(device: device)
    }
    for kp in vCopy.recursivelyAllWritableKeyPaths(to: Tensor?.self) {
      if let t = vCopy[keyPath: kp] { vCopy[keyPath: kp] = t.to(device: device) }
    }
    for kp in vCopy.recursivelyAllWritableKeyPaths(to: [Tensor].self) {
      vCopy[keyPath: kp] = vCopy[keyPath: kp].map { $0.to(device: device) }
    }

    self.m = mCopy
    self.v = vCopy
  }

  // MARK: - Step

  /// Applies one Adam update along the provided gradient direction.
  ///
  /// - Parameters:
  ///   - model: model parameters to update in place
  ///   - direction: gradient w.r.t. the model (∂L/∂θ)
  public func update(_ model: inout Model, along direction: Model.TangentVector) {
    step &+= 1

    // Optional LR decay (same functional form you used in SGD).
    let lr = learningRate * (1.0 / (1.0 + decay * Float(step)))
    let b1 = beta1
    let b2 = beta2
    let eps = epsilon

    // Update first/second moments per Tensor leaf:
    var newM = m
    var newV = v

    for kp in newM.recursivelyAllWritableKeyPaths(to: Tensor.self) {
      let g = direction[keyPath: kp]
      // m_t = β1 * m_{t-1} + (1-β1) * g_t
      newM[keyPath: kp] = newM[keyPath: kp].multiplying(b1)
        .adding(g.multiplying(1 - b1))
    }
    for kp in newV.recursivelyAllWritableKeyPaths(to: Tensor.self) {
      let g = direction[keyPath: kp]
      // v_t = β2 * v_{t-1} + (1-β2) * (g_t ⊙ g_t)
      let gg = g.multiplying(g)
      newV[keyPath: kp] = newV[keyPath: kp].multiplying(b2)
        .adding(gg.multiplying(1 - b2))
    }

    // Bias correction factors:
    let bc1 = 1 - Foundation.pow(b1, Float(step))
    let bc2 = 1 - Foundation.pow(b2, Float(step))

    // Compute the preconditioned step:  m̂ / (sqrt(v̂) + ε)
    var precond = Model.TangentVector.zero
    for kp in precond.recursivelyAllWritableKeyPaths(to: Tensor.self) {
      let mHat = newM[keyPath: kp].dividing(bc1)
      let vHat = newV[keyPath: kp].dividing(bc2)
      let denom = vHat.sqrt().adding(
        Tensor(eps, dtype: vHat.dtype ?? .float32, device: vHat.device))
      precond[keyPath: kp] = mHat.dividing(denom)
    }

    // Optional decoupled weight decay (AdamW): add λ * θ to the step before scaling by -lr.
    if adamW, weightDecay != 0 {
      var wd = model.differentiableVectorView  // same structure as parameters
      for kp in wd.recursivelyAllWritableKeyPaths(to: Tensor.self) {
        wd[keyPath: kp] = wd[keyPath: kp].multiplying(weightDecay)
      }
      precond = precond + wd
    }

    // Apply update: θ ← θ - lr * precond
    model.move(by: precond.scaled(by: -lr))

    // Commit moments.
    m = newM
    v = newV
  }
}
